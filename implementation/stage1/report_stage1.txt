Execution times:

For 1 process:
==================================================
ANALYSIS RESULTS
==================================================
DEBUG: 89447
ERROR: 18404
INFO: 152485
WARN: 43916
==================================================
Total time: 9.54s
==================================================

For 2 processes:
==================================================
ANALYSIS RESULTS
==================================================
DEBUG: 89447
ERROR: 18404
INFO: 152485
WARN: 43916
==================================================
Total time: 7.96s
==================================================

For 4 processes:
==================================================
ANALYSIS RESULTS
==================================================
DEBUG: 89447
ERROR: 18404
INFO: 152485
WARN: 43916
==================================================
Total time: 5.57s
==================================================

(8 processes not supported)

Speedup calculation:

For 1 processes: 9.54/9.54 = 1x speedup
For 2 processes: 9.54/7.96 = 1.198x speedup
For 4 processes: 9.54/5.57 = 1.712x speedup
For 8 processes: -

Efficiency analysis

For 1 process:   1/1     = 1
For 2 processes: 1.198/2 = 0.599
For 4 processes: 1.712/4 = 0.428
For 8 processes: -

Work distribution strategy:

The work distribution strategy used is chunking in the implementation.
Since we know the total number of log files to analyze and number of processes, we can divide it as equally as possible (not perfectly divisible if total files is not a multiple of number or processes).

We find the chunk size by dividing total files by number of processes and divide using that.
Once we divide into n chunks, 1st to (n-1)th chunk get equal number of files and the nth chunk gets the remaining(if not perfectly divisible)

Challenges faced:

I initially thought I should distribute the job among n-1 processes as the 1st process is the master group which allocated files.
I then realised it would be more efficient to give the master process the files too as it sits idle while the other processes analyse the files.

This not only sped up the analysis but also simplified the code as it allowed me to use the scatter() and gather() functions of mpi4py.

Observations:

It seems to be that as we increase the number of processes increases, the duration decreases linearly i.e. the plot of number of processes vs time taken to complete analysis falls on a straight line for these 3 points (1,2 and 4 processes).
However, the efficiency per process decreases as number of processes increases, but this is not linear. The decrease in efficiency reduces with more processes